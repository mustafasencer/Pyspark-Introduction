{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "from pyspark.ml.linalg import Vector\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF,StopWordsRemover,IDF,Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 554 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sc.stop()\n",
    "sc = SparkContext(\"local[2]\", \"Application\")\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents read in is: 1854\n",
      "Wall time: 18.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "path = \"C:\\\\Users\\\\lenovo\\\\Desktop\\\\PROJECTS\\\\SPARK\\\\newsgroup_data\\\\mini_newsgroups\\\\*\"\n",
    "newsGroupRowData = sc.wholeTextFiles(path)\n",
    "print(\"Number of documents read in is:\", newsGroupRowData.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('file:/C:/Users/lenovo/Desktop/PROJECTS/SPARK/newsgroup_data/mini_newsgroups/176881',\n",
       "  'Xref: cantaloupe.srv.cs.cmu.edu talk.politics.misc:176881 talk.abortion:118611\\nPath: cantaloupe.srv.cs.cmu.edu!crabapple.srv.cs.cmu.edu!fs7.ece.cmu.edu!europa.eng.gtefsd.com!gatech!udel!wupost!uunet!olivea!sgigate!odin!fido!zola!eno.esd.sgi.com!cj\\nFrom: cj@eno.esd.sgi.com (C.J. Silverio)\\nNewsgroups: talk.politics.misc,talk.abortion\\nSubject: Re: Tieing Abortion to Health Reform -- Is Clinton Nuts?\\nMessage-ID: <h3lnrb8@zola.esd.sgi.com>\\nDate: 5 Apr 93 20:24:11 GMT\\nReferences: <1pdbocINNg8g@geraldo.cc.utexas.edu> <1pdi47$3mo@agate.berkeley.edu> <sandvik-310393202727@sandvik-kent.apple.com> <1993Apr1.181711.9172@ringer.cs.utsa.edu> <1993Apr2.230831.18332@wdl.loral.com> <C4z3xw.3EF@news.cso.uiuc.edu>\\nSender: news@zola.esd.sgi.com (Net News)\\nReply-To: cj@sgi.com\\nFollowup-To: talk.politics.misc\\nOrganization: SGI Developer Docudramas\\nLines: 27\\n\\n\\nIn article <C4z3xw.3EF@news.cso.uiuc.edu>, parker@ehsn21.cen.uiuc.edu writes:\\n| I like the way people call it \"cruel and unusual punishment\", as if\\n| imprisonment isn\\'t cruel, too.  Lethal injection pales in comparison.\\n| And, they have a death sentence because they were convicted of a cruel\\n| and unusual *crime*.\\n\\nIt\\'s not what they did that matters.  It\\'s what *you* do and\\nwhat *I* do and what *we* do in response that matters.  Do we\\nlessen ourselves by killing in response to killing?  It\\'s\\nvengeance.  That\\'s all.  It\\'s no deterrent.  It serves no\\npurpose but to slake somebody\\'s blood lust.\\n\\n| It would be nice, though, if we never convicted someone of a crime they\\n| didn\\'t commit, and it would make the death penalty much more justifiable.\\n\\nYeah yeah yeah... and sure would be nice if we didn\\'t apply the\\ndeath penalty disproportionately to minorities.  I\\'ll revisit my\\nopinion on the death penalty when there are more whites up for\\nit than blacks.  I.e., when hell freezes over.\\n\\n---\\nC J Silverio\\tcj@sgi.com\\tceej@well.sf.ca.us\\n\"The people causing the trouble were socialists and homosexuals,\\nthe typical sort of person who opposes us.\"  --Don Treshman, \\nex-Klansman, leader of the \"pro-life\" group Rescue America, \\non BBC TV, 2 April 1993.\\n')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsGroupRowData.takeSample(False, 1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['file:/C:/Users/lenovo/Desktop/PROJECTS/SPARK/newsgroup_data/mini_newsgroups/20896', 'file:/C:/Users/lenovo/Desktop/PROJECTS/SPARK/newsgroup_data/mini_newsgroups/54171', 'file:/C:/Users/lenovo/Desktop/PROJECTS/SPARK/newsgroup_data/mini_newsgroups/59458', 'file:/C:/Users/lenovo/Desktop/PROJECTS/SPARK/newsgroup_data/mini_newsgroups/176881', 'file:/C:/Users/lenovo/Desktop/PROJECTS/SPARK/newsgroup_data/mini_newsgroups/55060']\n",
      "Wall time: 32.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "filepaths = newsGroupRowData.map(lambda filepath : filepath[0])\n",
    "print(filepaths.takeSample(False, 5, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Xref: cantaloupe.srv.cs.cmu.edu talk.politics.misc:176881 talk.abortion:118611\\nPath: cantaloupe.srv.cs.cmu.edu!crabapple.srv.cs.cmu.edu!fs7.ece.cmu.edu!europa.eng.gtefsd.com!gatech!udel!wupost!uunet!olivea!sgigate!odin!fido!zola!eno.esd.sgi.com!cj\\nFrom: cj@eno.esd.sgi.com (C.J. Silverio)\\nNewsgroups: talk.politics.misc,talk.abortion\\nSubject: Re: Tieing Abortion to Health Reform -- Is Clinton Nuts?\\nMessage-ID: <h3lnrb8@zola.esd.sgi.com>\\nDate: 5 Apr 93 20:24:11 GMT\\nReferences: <1pdbocINNg8g@geraldo.cc.utexas.edu> <1pdi47$3mo@agate.berkeley.edu> <sandvik-310393202727@sandvik-kent.apple.com> <1993Apr1.181711.9172@ringer.cs.utsa.edu> <1993Apr2.230831.18332@wdl.loral.com> <C4z3xw.3EF@news.cso.uiuc.edu>\\nSender: news@zola.esd.sgi.com (Net News)\\nReply-To: cj@sgi.com\\nFollowup-To: talk.politics.misc\\nOrganization: SGI Developer Docudramas\\nLines: 27\\n\\n\\nIn article <C4z3xw.3EF@news.cso.uiuc.edu>, parker@ehsn21.cen.uiuc.edu writes:\\n| I like the way people call it \"cruel and unusual punishment\", as if\\n| imprisonment isn\\'t cruel, too.  Lethal injection pales in comparison.\\n| And, they have a death sentence because they were convicted of a cruel\\n| and unusual *crime*.\\n\\nIt\\'s not what they did that matters.  It\\'s what *you* do and\\nwhat *I* do and what *we* do in response that matters.  Do we\\nlessen ourselves by killing in response to killing?  It\\'s\\nvengeance.  That\\'s all.  It\\'s no deterrent.  It serves no\\npurpose but to slake somebody\\'s blood lust.\\n\\n| It would be nice, though, if we never convicted someone of a crime they\\n| didn\\'t commit, and it would make the death penalty much more justifiable.\\n\\nYeah yeah yeah... and sure would be nice if we didn\\'t apply the\\ndeath penalty disproportionately to minorities.  I\\'ll revisit my\\nopinion on the death penalty when there are more whites up for\\nit than blacks.  I.e., when hell freezes over.\\n\\n---\\nC J Silverio\\tcj@sgi.com\\tceej@well.sf.ca.us\\n\"The people causing the trouble were socialists and homosexuals,\\nthe typical sort of person who opposes us.\"  --Don Treshman, \\nex-Klansman, leader of the \"pro-life\" group Rescue America, \\non BBC TV, 2 April 1993.\\n']\n",
      "Wall time: 34.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "text = newsGroupRowData.map(lambda filepath :  filepath[1])\n",
    "print(text.takeSample(False, 1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Newsgroups: comp.os.ms-windows.misc\\nPath: cantaloupe.srv.cs.cmu.edu!magnesium.club.cc.cmu.edu!news.sei.cmu.edu!cis.ohio-state.edu!zaphod.mps.ohio-state.edu!caen!batcomputer!munnari.oz.au!titan!pcies2.trl.OZ.AU!d.gellert\\nFrom: d.gellert@trl.oz.au (Dennis Gellert)\\nSubject: Re: Why is my mouse so JUMPY? (MS MOUSE)\\nMessage-ID: <d.gellert.14.0@trl.oz.au>\\nLines: 25\\nSender: root@trl.oz.au (System PRIVILEGED Account)\\nOrganization: Telecom Research\\nReferences: <_c$@byu.edu> <1993Apr23.140123.5018@cti.com>\\nDate: Tue, 27 Apr 1993 23:02:39 GMT\\n\\nIn article <1993Apr23.140123.5018@cti.com> rlister@cti.com (Russell Lister) writes:\\n>From: rlister@cti.com (Russell Lister)\\n>Subject: Re: Why is my mouse so JUMPY? (MS MOUSE)\\n>Date: Fri, 23 Apr 1993 14:01:23 GMT\\n\\n>ecktons@ucs.byu.edu (Sean Eckton) writes:\\n\\n>>I have a Microsoft Serial Mouse and am using mouse.com 8.00 (was using 8.20 \\n>>I think, but switched to 8.00 to see if it was any better).  Vertical motion \\n>>is nice and smooth, but horizontal motion is so bad I sometimes can't click \\n>>on something because my mouse jumps around.  I can be moving the mouse to \\n>>the right with relatively uniform motion and the mouse will move smoothly \\n>>for a bit, then jump to the right, then move smoothly for a bit then jump \\n>>again (maybe this time to the left about .5 inch!).  This is crazy!  I have \\n>>never had so much trouble with a mouse before.  Anyone have any solutions?  \\n\\n>>Does Microsoft think they are what everyone should be? <- just venting steam!\\n\\nI've seen this problem several times. It was always the result of the little \\nrollers inside the mouse becomming dirty- they are good at collecting grime.\\nthe solution is simple: remove the ball to reveal the two rollers. \\nCarefully clean them and the ball.\\nDennis\\n\\n\\n\", 'Newsgroups: comp.os.ms-windows.misc\\nPath: cantaloupe.srv.cs.cmu.edu!das-news.harvard.edu!noc.near.net!howland.reston.ans.net!agate!news.ucdavis.edu!othello.ucdavis.edu!ez003045\\nFrom: ez003045@othello.ucdavis.edu (James E. Lee)\\nSubject: Program Manager problem\\nMessage-ID: <C661C2.2HA@ucdavis.edu>\\nSender: usenet@ucdavis.edu (News Administrator)\\nOrganization: University of California, Davis\\nX-Newsreader: Tin 1.1 PL3\\nDate: Tue, 27 Apr 1993 23:31:14 GMT\\nLines: 19\\n\\n\\nDoes anyone know how to configure a DOS app in Progman\\nso that only one instance of it can be running at a time?\\n\\nI\\'d really appreciate some help on how to do this.  I\\nwould prefer responses through email if it\\'s not a big \\ndeal, or at least through email _as well as_ posting.\\n\\nThank you!\\n\\n--\\nJames E. Lee  \\njelee@hamlet.ucdavis.edu \\n\\n\"I swear--by my life and my love of it--that I will never live for the\\nsake of another man, nor ask another man to live for mine.\"\\n                                            -John Galt\\n                                          \"Atlas Shrugged\"     \\n                                      \\n', 'Path: cantaloupe.srv.cs.cmu.edu!magnesium.club.cc.cmu.edu!news.sei.cmu.edu!cis.ohio-state.edu!zaphod.mps.ohio-state.edu!howland.reston.ans.net!ux1.cso.uiuc.edu!newsrelay.iastate.edu!iscsvax.uni.edu!holmes7000\\nFrom: holmes7000@iscsvax.uni.edu\\nNewsgroups: comp.os.ms-windows.misc\\nSubject: WIn 3.0 ICON HELP PLEASE!\\nMessage-ID: <1993Apr27.193102.12730@iscsvax.uni.edu>\\nDate: 27 Apr 93 19:31:02 -0600\\nOrganization: University of Northern Iowa\\nLines: 10\\n\\nI have win 3.0 and downloaded several icons and BMP\\'s but I can\\'t figure out\\nhow to change the \"wallpaper\" or use the icons.  Any help would be appreciated.\\n\\n\\nThanx,\\n\\n-Brando\\n\\nPS Please E-mail me\\n\\n', 'Newsgroups: comp.os.ms-windows.misc\\nPath: cantaloupe.srv.cs.cmu.edu!crabapple.srv.cs.cmu.edu!bb3.andrew.cmu.edu!news.sei.cmu.edu!cis.ohio-state.edu!pacific.mps.ohio-state.edu!zaphod.mps.ohio-state.edu!swrinde!gatech!concert!decwrl!world!paladin\\nFrom: paladin@world.std.com (Thomas G Schlatter)\\nSubject: Newsgroup archive\\nMessage-ID: <C66DIu.96t@world.std.com>\\nOrganization: The World Public Access UNIX, Brookline, MA\\nDate: Wed, 28 Apr 1993 03:54:29 GMT\\nLines: 7\\n\\nIs this newsgroup archived anywhere beyond the normal expiration\\ndates, say for the last 6 months or more?\\n\\nThanks,\\nTom\\npaladin@world.std.com\\n\\n', \"Xref: cantaloupe.srv.cs.cmu.edu comp.os.msdos.apps:10855 comp.os.msdos.misc:13288 comp.os.ms-windows.misc:10016\\nPath: cantaloupe.srv.cs.cmu.edu!crabapple.srv.cs.cmu.edu!bb3.andrew.cmu.edu!news.sei.cmu.edu!cis.ohio-state.edu!zaphod.mps.ohio-state.edu!howland.reston.ans.net!agate!netsys!pagesat!news.cerf.net!crash!ryptyde!jdriver\\nNewsgroups: comp.os.msdos.apps,comp.os.msdos.misc,comp.os.ms-windows.misc\\nSubject: Is SMARTDRV.EXE causing bad sectors on my hd?\\nFrom: jdriver@netlink.cts.com (John Driver)\\nMessage-ID: <uo6o3B3w165w@netlink.cts.com>\\nDate: Tue, 27 Apr 93 20:18:05 PDT\\nOrganization: NetLink Online Communications, San Diego CA\\nLines: 47\\n\\n        I am having something very unusual happen.  First \\nsome background on my system.  I have a Mitsubishi 63 meg Hard Drive, \\nand am running Smartdrv (the version that comes with Windows 3.1) on \\nit.  I rarely use Windows.  I use a program called Disk Technician \\nGold v1.14 to do diagnostics live time on my hard drive.  It works by \\nhaving a device driver detect whenever more than one read is \\nnecessary for a file, or if there is anything else is wrong with it, \\nand minor problems are fully checked out upon rebooting.  My hard \\ndrive is notorious for bad sectors.  I usually end up with 8 new bad \\nsectors a week.\\n \\n        Here's what happened:  I ran a program, and DTG broke in with \\nan Emergency Warning and recommended I reboot.  It gave me this \\nmessage twice before the program was fully loaded.  I exited the program \\nand did just this.  DTG went through its bootup process, examining \\nfor new errors etc., and a screen popped up and said something about \\nsectors for a brief period of time.\\n \\n        I then went back to the program, executed it again, and the \\nexact same error was detected.  I rebooted and tried again, and the \\nsame error happened again.  So, I removed DTG from memory, and went \\nto the program to see if I could detect anything wrong.  Sure enough \\nthere was a number of read attempts.  So I rebooted and reloaded DTG, \\nbut removed the cache.  I executed the program.  No read errors, \\neither audible or detected by DTG.  I quit the program, loaded the \\ncache, and ran the program again.  The errors were detected.\\n \\n        Ok, so the errors are there, and DTG detects but doesn't fix \\nthem, when the cache is loaded.  When the cache is not loaded there are \\nno errors.  So, to see if the cache was interfering with any other \\nfiles, I went into xtree gold and tagged all files, and searched them \\nfor a random string (in other words, I wanted the program to \\ncompletely read every file on my hard drive).  Before I got through \\nthe c's DTG had detected at least six errors and recommended I reboot.\\n \\n        Does anybody, have any idea why Smartdrv is causing misreads on \\nmy hard drive?  Oh, there are exactly two misreads per file, and 1 in \\nabout every 100 files are affected.  \\n \\n        I originally posted this message to Disk Technician Corp.'s \\nsystem, but I figured someone out in netland may know enough about \\nsmartdrv to help me out.\\n\\n--                    \\nINTERNET:  jdriver@netlink.cts.com (John Driver)\\nUUCP:   ...!ryptyde!netlink!jdriver\\nNetLink Online Communications * Public Access in San Diego, CA (619) 453-1115\\n\"]\n",
      "Wall time: 2.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "id = filepaths.map(lambda filepath: filepath.split(\"//\")[-1])\n",
    "print(id.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 14.0 failed 1 times, most recent failure: Lost task 0.0 in stage 14.0 (TID 24, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 177, in main\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in process\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\pyspark\\rdd.py\", line 1339, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<timed exec>\", line 1, in <lambda>\nIndexError: list index out of range\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1167)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:641)\r\n\tat java.base/java.lang.Thread.run(Thread.java:844)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:446)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.base/java.lang.Thread.run(Thread.java:844)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 177, in main\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in process\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\pyspark\\rdd.py\", line 1339, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<timed exec>\", line 1, in <lambda>\nIndexError: list index out of range\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1167)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:641)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1342\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1343\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1345\u001b[0m             \u001b[0mitems\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36mrunJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m    990\u001b[0m         \u001b[1;31m# SparkContext#runJob.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 992\u001b[1;33m         \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    993\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1133\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 14.0 failed 1 times, most recent failure: Lost task 0.0 in stage 14.0 (TID 24, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 177, in main\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in process\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\pyspark\\rdd.py\", line 1339, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<timed exec>\", line 1, in <lambda>\nIndexError: list index out of range\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1167)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:641)\r\n\tat java.base/java.lang.Thread.run(Thread.java:844)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:446)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.base/java.lang.Thread.run(Thread.java:844)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 177, in main\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in process\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\pyspark\\rdd.py\", line 1339, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<timed exec>\", line 1, in <lambda>\nIndexError: list index out of range\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1167)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:641)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "topics = filepaths.map(lambda filepath: filepath.split(\"\\\\\")[-2])\n",
    "print(topics.take(5))\n",
    "print(topics.distict().take(20))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
