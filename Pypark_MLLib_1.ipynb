{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"C:\\\\spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.56.1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>first_example</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7df129ac88>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"first_example\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training = spark.createDataFrame([\n",
    "    (0, \"s p a r k\", 1.0),\n",
    "    (1, \"h a d o o p\", 0.0),\n",
    "    (2, \"s p a\", 1.0),\n",
    "    (3, \"h a d\", 0.0),\n",
    "    (4, \"h p a\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol= \"text\", outputCol = \"words\")\n",
    "hashingTF = HashingTF(inputCol = tokenizer.getOutputCol(), outputCol = \"features\")\n",
    "lr = LogisticRegression(maxIter = 10, regParam = 0.01)\n",
    "pipeline = Pipeline(stages = [tokenizer, hashingTF, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression parameters: \n",
      "aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n",
      "family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto)\n",
      "featuresCol: features column name. (default: features)\n",
      "fitIntercept: whether to fit an intercept term. (default: True)\n",
      "labelCol: label column name. (default: label)\n",
      "maxIter: max number of iterations (>= 0). (default: 100, current: 10)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0, current: 0.01)\n",
      "standardization: whether to standardize the training features before fitting the model. (default: True)\n",
      "threshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"LogisticRegression parameters: \\n\" + lr.explainParams() + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = pipeline.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = spark.createDataFrame([\n",
    "    (5, \"s r k\"),\n",
    "    (6, \"h d p\"),\n",
    "    (7, \"s p\"),\n",
    "    (8, \"h o\"),\n",
    "    (9, \"h d p\"),\n",
    "    (10, \"p a\")\n",
    "],  [\"id\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---------+--------------------+--------------------+--------------------+----------+\n",
      "| id| text|    words|            features|       rawPrediction|         probability|prediction|\n",
      "+---+-----+---------+--------------------+--------------------+--------------------+----------+\n",
      "|  5|s r k|[s, r, k]|(262144,[94533,18...|[-4.1961909513595...|[0.01482957788594...|       1.0|\n",
      "|  6|h d p|[h, d, p]|(262144,[15554,27...|[4.27395562512471...|[0.98626470012802...|       0.0|\n",
      "|  7|  s p|   [s, p]|(262144,[94533,21...|[-3.1856083796668...|[0.03971091129109...|       1.0|\n",
      "|  8|  h o|   [h, o]|(262144,[15554,25...|[4.23296628247586...|[0.98569822062977...|       0.0|\n",
      "|  9|h d p|[h, d, p]|(262144,[15554,27...|[4.27395562512471...|[0.98626470012802...|       0.0|\n",
      "| 10|  p a|   [p, a]|(262144,[213268,2...|[0.00540714716231...|[0.50135178349704...|       0.0|\n",
      "+---+-----+---------+--------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction = model.transform(test)\n",
    "prediction.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(id=5, text='s r k', prediction=1.0)\n",
      "Row(id=6, text='h d p', prediction=0.0)\n",
      "Row(id=7, text='s p', prediction=1.0)\n",
      "Row(id=8, text='h o', prediction=0.0)\n",
      "Row(id=9, text='h d p', prediction=0.0)\n",
      "Row(id=10, text='p a', prediction=0.0)\n"
     ]
    }
   ],
   "source": [
    "selected = prediction.select(\"id\", \"text\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    print(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
